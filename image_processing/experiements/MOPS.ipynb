{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URL : https://github.com/jpatts/image_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(\"`{}` not cannot be loaded\".format(image_path))\n",
    "    return img\n",
    "\n",
    "def draw_matches(matches, img_left, img_right, verbose=False):\n",
    "    # Determine the max height\n",
    "    height = max(img_left.shape[0], img_right.shape[0])\n",
    "    # Width is the two images side-by-side\n",
    "    width = img_left.shape[1] + img_right.shape[1]\n",
    "\n",
    "    img_out = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    # Place the images in the empty image \n",
    "    img_out[0:img_left.shape[0], 0:img_left.shape[1], :] = img_left\n",
    "    img_out[0:img_right.shape[0], img_left.shape[1]:, :] = img_right\n",
    "\n",
    "    # The right image coordinates are offset since the image is no longer at (0,0)\n",
    "    ow = img_left.shape[1]\n",
    "   \n",
    "    #Draw a line between the matched pairs in green\n",
    "    for p1,p2 in matches:\n",
    "        p1o = (int(p1[1]), int(p1[0]))\n",
    "        p2o = (int(p2[1] + ow), int(p2[0]))\n",
    "        color = list(np.random.random(size=3) * 256)\n",
    "        cv2.line(img_out, p1o, p2o, color, thickness=2)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Press enter to continue ... \")\n",
    "        cv2.imshow(\"matches\", img_out)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "    cv2.imwrite(\"MOPS_matches.png\", img_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.time()\n",
    "        self.secs = self.end - self.start\n",
    "        self.mins = self.secs // 60\n",
    "        self.msecs = self.secs * 1000  # millisecs\n",
    "        if self.verbose:\n",
    "            print('elapsed time: {:.2f} s ({:.4f} ms)'.format(self.secs, self.msecs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARRIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse as ap\n",
    "\n",
    "def harris(img, sigma=1, threshold=0.01):\n",
    "    height, width = img.shape\n",
    "    shape = (height, width)\n",
    "    # Calculate the dx,dy gradients of the image (np.gradient doesnt work)\n",
    "    dx = cv2.Sobel(img, cv2.CV_64F,1,0,ksize=5)\n",
    "    dy = cv2.Sobel(img, cv2.CV_64F,0,1,ksize=5)\n",
    "    # Get angle for rotation\n",
    "    _, ang = cv2.cartToPolar(dx, dy, angleInDegrees=True)\n",
    "    # Square the derivatives (A,B,C of H) and apply apply gaussian filters to each\n",
    "    sigma = (sigma, sigma)\n",
    "    Ixx = cv2.GaussianBlur(dx * dx, sigma, 0)\n",
    "    Ixy = cv2.GaussianBlur(dx * dy, sigma, 0)\n",
    "    Iyy = cv2.GaussianBlur(dy * dy, sigma, 0)\n",
    "\n",
    "    H = np.array([[Ixx, Ixy], [Ixy, Iyy]])\n",
    "    # Find the determinate\n",
    "    num = (H[0, 0] * H[1, 1]) - (H[0, 1] * H[1, 0])\n",
    "    # Find the trace\n",
    "    denom = H[0,0] + H[1,1]\n",
    "    # Find the response using harmonic mean of the eigenvalues (Brown et. al. variation) \n",
    "    R = np.nan_to_num(num / denom)\n",
    "    \n",
    "    # Adaptive non-maximum suppression, keep the top 1% of values and remove non-maximum points in a 9x9 neighbourhood\n",
    "    R_flat = R[:].flatten()\n",
    "    # Get number of values in top threshold %\n",
    "    N = int(len(R_flat) * threshold)\n",
    "    # Get values in top threshold %\n",
    "    top_k_percentile = np.partition(R_flat, -N)[-N:]\n",
    "    # Find lowest value in top threshold %\n",
    "    minimum = np.min(top_k_percentile)\n",
    "    # Set all values less than this to 0\n",
    "    R[R < minimum] = 0\n",
    "    # Set non-maximum points in an SxS neighbourhood to 0\n",
    "    s = 9\n",
    "    for h in range(R.shape[0] - s):\n",
    "        for w in range(R.shape[1] - s):\n",
    "            maximum = np.max(R[h:h+s+1, w:w+s+1])\n",
    "            for i in range(h, h+s+1):\n",
    "                for j in range(w, w+s+1):\n",
    "                    if R[i, j] != maximum:\n",
    "                        R[i, j] = 0\n",
    "                        \n",
    "    # Return harris corners in [H, W, R] format\n",
    "    features = list(np.where(R > 0))\n",
    "    features.append(ang[np.where(R > 0)])\n",
    "    corners = zip(*features)\n",
    "    return list(corners)\n",
    "\n",
    "def draw_corners(corners, img, name):\n",
    "    for h, w, r in corners:\n",
    "        cv2.circle(img, (w, h), 2, (0, 0, 255))\n",
    "\n",
    "    cv2.imwrite(name + '.png', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, math, pywt\n",
    "import numpy as np\n",
    "import argparse as ap\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import warp, AffineTransform\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "def ransac(pts1, pts2, img_l, img_r, max_iters=1000, epsilon=1):\n",
    "    best_matches = []\n",
    "    # Number of samples\n",
    "    N = 4\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Get 4 random samples from features\n",
    "        idx = np.random.randint(0, len(pts1) - 1, N)\n",
    "        src = pts1[idx]\n",
    "        dst = pts2[idx]\n",
    "\n",
    "        # Calculate the homography matrix H\n",
    "        H = cv2.getPerspectiveTransform(src, dst)\n",
    "        Hp = cv2.perspectiveTransform(pts1[None], H)[0]\n",
    "\n",
    "        # Find the inliers by computing the SSD(p',Hp) and saving inliers (feature pairs) that are SSD(p',Hp) < epsilon\n",
    "        inliers = []\n",
    "        for i in range(len(pts1)):\n",
    "            ssd = np.sum(np.square(pts2[i] - Hp[i]))\n",
    "            if ssd < epsilon:\n",
    "                inliers.append([pts1[i], pts2[i]])\n",
    "        \n",
    "        # Keep the largest set of inliers and the corresponding homography matrix\n",
    "        if len(inliers) > len(best_matches):\n",
    "            best_matches = inliers\n",
    "    \n",
    "    return best_matches\n",
    "\n",
    "# Plotting tool for convenience\n",
    "def plot(plots):\n",
    "    fig = plt.figure()\n",
    "    N = len(plots)\n",
    "    x = math.ceil(math.sqrt(N))\n",
    "    y = math.ceil(math.sqrt(N))\n",
    "    for i in range(N):\n",
    "        fig.add_subplot(x,y,i+1).imshow(plots[i])\n",
    "    plt.show()\n",
    "\n",
    "def mops(img, truth, win_size, h, w, r):\n",
    "    H, W = img.shape\n",
    "    offset = win_size // 2\n",
    "    \n",
    "    # Draw line for angle of gradient (for debugging)\n",
    "    #length = 150\n",
    "    #h2 =  int(h - length * math.cos(math.radians(r)))\n",
    "    #w2 =  int(w - length * math.sin(math.radians(r)))\n",
    "    #cv2.line(img, (w, h), (w2, h2), (0,255,0), 2)\n",
    "\n",
    "    # Rotate image s.t. gradient angle of feature is origin\n",
    "    M = cv2.getRotationMatrix2D((w, h), -1*r, 1)\n",
    "    img_rot = cv2.warpAffine(img, M, (W, H), flags=cv2.INTER_NEAREST)\n",
    "\n",
    "    # Get 40x40 window around feature\n",
    "    win = img_rot[h-offset:h+offset, w-offset:w+offset]\n",
    "\n",
    "    # Size (s,s) of each sample region\n",
    "    s = win_size // 8\n",
    "    # Prefiltering (N,N) -> (N//s, N//s)\n",
    "    i = 0\n",
    "    rows = []\n",
    "    while i < win_size:\n",
    "        j = 0\n",
    "        cols = []\n",
    "        while j < win_size:\n",
    "            # Sample (s,s) region from window\n",
    "            sample = win[i:i+s, j:j+s]\n",
    "            # Downsample (s,s) region to a single value\n",
    "            sample = np.sum(sample) / (s*s)\n",
    "            cols.append(sample)\n",
    "            j += s\n",
    "        rows.append(cols)\n",
    "        i += s\n",
    "    \n",
    "    feature = np.array(rows)\n",
    "    # Normalize\n",
    "    feature = (feature - np.mean(feature)) / np.std(feature)\n",
    "    # Haar wave transform\n",
    "    coeffs = pywt.dwt2(feature, 'haar')\n",
    "    feature = pywt.idwt2(coeffs, 'haar')\n",
    "    #plot([img, img_rot, truth, win, feature])\n",
    "    return feature\n",
    "\n",
    "def get_matches(ft1, ft2, left_match, right_match, img_left, img_right, max_matches, win_size):\n",
    "    potential_matches = []\n",
    "    offset = win_size // 2\n",
    "\n",
    "    # Feature descriptor with brute-force matching\n",
    "    for h1, w1, r1 in ft1:\n",
    "        \n",
    "        # Copy image as to not modify it by reference\n",
    "        img_left_tmp = np.copy(img_left)\n",
    "\n",
    "        # Copy coordinates incase changed by border\n",
    "        h1_tmp = h1\n",
    "        w1_tmp = w1\n",
    "\n",
    "        # Get 40x40 window around feature\n",
    "        win_left = img_left[h1-offset:h1+offset, w1-offset:w1+offset]\n",
    "\n",
    "        # Handle border points\n",
    "        if win_left.shape != (win_size, win_size):\n",
    "            diff_h, diff_w = np.subtract(win_left.shape, (win_size, win_size))\n",
    "            p_h = abs(diff_h // 2)\n",
    "            p_w = abs(diff_w // 2)\n",
    "            img_left_tmp = cv2.copyMakeBorder(img_left_tmp, p_h, p_h, p_w, p_w, cv2.BORDER_REFLECT)\n",
    "            h1 += p_h\n",
    "            w1 += p_w\n",
    "            win_left = img_left_tmp[h1-offset:h1+offset, w1-offset:w1+offset]\n",
    "        \n",
    "        # Run multiscale oriented patches descriptor\n",
    "        feature_left = mops(img_left_tmp, win_left, win_size, h1, w1, r1)\n",
    "\n",
    "        lowest_dist = math.inf\n",
    "        potential_match = ()\n",
    "        for h2, w2, r2 in ft2:\n",
    "\n",
    "            # Copy image as to not modify it by reference\n",
    "            img_right_tmp = np.copy(img_right)\n",
    "\n",
    "            # Copy coordinates incase changed by border\n",
    "            h2_tmp = h2\n",
    "            w2_tmp = w2\n",
    "\n",
    "            # Get 40x40 window around feature\n",
    "            win_right = img_right[h2-offset:h2+offset, w2-offset:w2+offset]\n",
    "\n",
    "            # Handle border points\n",
    "            if win_right.shape != (win_size, win_size):\n",
    "                diff_h, diff_w = np.subtract(win_right.shape, (win_size, win_size))\n",
    "                p_h = abs(diff_h // 2)\n",
    "                p_w = abs(diff_w // 2)\n",
    "                img_right_tmp = cv2.copyMakeBorder(img_right_tmp, p_h, p_h, p_w, p_w, cv2.BORDER_REFLECT)\n",
    "                h2 += p_h\n",
    "                w2 += p_w\n",
    "                win_right = img_right_tmp[h2-offset:h2+offset, w2-offset:w2+offset]\n",
    "\n",
    "            # Run multiscale oriented patches descriptor\n",
    "            feature_right = mops(img_right_tmp, win_right, win_size, h2, w2, r2)\n",
    "            \n",
    "            # Check distance between features\n",
    "            curr_dist = np.linalg.norm(feature_left - feature_right)\n",
    "            if curr_dist < lowest_dist:\n",
    "                lowest_dist = curr_dist\n",
    "                potential_match = ([h1_tmp, w1_tmp, r1], [h2_tmp, w2_tmp, r2], curr_dist)\n",
    "        \n",
    "        potential_matches.append(potential_match)\n",
    "        \n",
    "    # Sort matches from smallest distance up\n",
    "    matches = sorted(potential_matches, key=itemgetter(2))\n",
    "    for match in matches:\n",
    "        # Ensure no duplicates\n",
    "        if match[0][0:2] not in left_match and match[1][0:2] not in right_match:\n",
    "            # Add to matches\n",
    "            left_match.append(match[0][0:2])\n",
    "            right_match.append(match[1][0:2])\n",
    "            # Remove from potential matches\n",
    "            ft1.remove(tuple(match[0]))\n",
    "            ft2.remove(tuple(match[1]))\n",
    "    \n",
    "    # Recursively keep going until every point has a match\n",
    "    while(len(left_match) < max_matches and len(right_match) < max_matches):\n",
    "        print('anotha one')\n",
    "        get_matches(ft1, ft2, left_match, right_match, img_left, img_right, max_matches, win_size)\n",
    "\n",
    "    return np.array(left_match, dtype=np.float32), np.array(right_match, dtype=np.float32)\n",
    "\n",
    "def main(image):\n",
    "    # Load the image\n",
    "    img_left_clr   = read_image(image)\n",
    "   \n",
    "    # Perform a some transforms on it for feature matching \n",
    "    tform = AffineTransform(scale=(0.8, 0.8), rotation=0.2, translation=(20, -10))\n",
    "    img_right_clr = warp(img_left_clr, tform.inverse, output_shape=img_left_clr.shape[:2])\n",
    "    img_right_clr = np.uint8(img_right_clr * 255) \n",
    "\n",
    "    # Convert to grayscale\n",
    "    img_left = cv2.cvtColor(img_left_clr, cv2.COLOR_BGR2GRAY)\n",
    "    img_right = cv2.cvtColor(img_right_clr, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    print(\"Getting the features from the Harris Detector\")\n",
    "    ftL = harris(img_left, sigma=3, threshold=0.01)\n",
    "    draw_corners(ftL, img_left_clr, 'corners_left')\n",
    "    ftR = harris(img_right, sigma=3, threshold=0.01)\n",
    "    draw_corners(ftR, img_right_clr, 'corners_right')\n",
    "    \n",
    "    print(\"  -- Number of features (left): \", len(ftL))\n",
    "    print(\"  -- Number of features (right): \", len(ftR))\n",
    "    print(\"Finding the best matches between images\")\n",
    "    with Timer(verbose=True) as t:\n",
    "        max_matches = min(len(ftL), len(ftR))\n",
    "        ptsL,ptsR = get_matches(ftL, ftR, [], [], img_left, img_right, max_matches, win_size=50)\n",
    "        \n",
    "        print(\" -- Number of matches = \", len(ptsL))\n",
    "        assert len(ptsL) == len(ptsR)\n",
    "\n",
    "    print(\"Performing RANSAC\")\n",
    "    matches = ransac(ptsL,ptsR, img_left, img_right, 1000, 100)\n",
    "    print(\" -- Number of pruned matches = \", len(matches))\n",
    "    \n",
    "    draw_matches(matches, img_left_clr, img_right_clr)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = ap.ArgumentParser()\n",
    "#     parser.add_argument(\"image\", \n",
    "#                         default=\"imgs/left.png\",\n",
    "#                         help=\"Path to left image\") \n",
    "    \n",
    "#     parser.add_argument(\"-w\", \n",
    "#                         \"--win_size\", \n",
    "#                         default=50,\n",
    "#                         type=int,\n",
    "#                         help=\"Window size for your feature detector algorithm\") \n",
    "    \n",
    "#     parser.add_argument(\"-i\", \n",
    "#                         \"--max_iters\", \n",
    "#                         default=1000,\n",
    "#                         type=int,\n",
    "#                         help=\"Maximum iterations to perform RANSAC\") \n",
    "    \n",
    "#     parser.add_argument(\"-e\", \n",
    "#                         \"--epsilon\", \n",
    "#                         default=100,\n",
    "#                         type=float,\n",
    "#                         help=\"SSD epsilon threshold for RANSAC\") \n",
    "   \n",
    "#     parser.add_argument(\"-v\", \n",
    "#                         \"--verbose\", \n",
    "#                         help=\"Turn on debugging statements\",\n",
    "#                         action=\"store_true\") \n",
    "\n",
    "#     args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the features from the Harris Detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-23877ef0da89>:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "  R = np.nan_to_num(num / denom)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Number of features (left):  80\n",
      "  -- Number of features (right):  72\n",
      "Finding the best matches between images\n",
      "anotha one\n",
      "anotha one\n",
      "anotha one\n",
      "anotha one\n",
      " -- Number of matches =  72\n",
      "elapsed time: 15.44 s (15437.1989 ms)\n",
      "Performing RANSAC\n",
      " -- Number of pruned matches =  22\n"
     ]
    }
   ],
   "source": [
    "main(\"maksssksksss513.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
